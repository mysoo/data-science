**<statistical language model>[정규표현식]**

-language model : 문장에서 확률을 할당하는 모델. 모델이 input으로 받은 문장이 어떤 것은 말이 되고, 어떤 것은 말이 안된다는 것을 판단하게 만들어주는 모델 // 보편적으로 앞 문장들을 통해 다음 단어를 예측

-statistical language model[정규표현식] : 단어 시퀀스에 확률을 할당하는 모델 // **조건부확률**로 구하는 모델. 앞에 단어들이 있을 때, 다음 단어가 어떤 것이 나올 것인지 이에 대한 조건부확률을 통해 구함. Ex>n번째 단어 예측 -> x(1)부터 x(n-1)까지 단어가 주어졌을 때, 다음으로 어떤 단어가 나올 확률이 가장 높은가?

​	-조건부확률로 어떻게 단어를 구할 수 있을까? : 단어에 대한 확률은 **count** 기반으로 계산됨. Ex>백	만개의 문장들로 구성된 train data set에서 한 문장이 100번 등장하고, 그 다음으로 또 다른 문장이 	30번 등장 -> 조건부 확률 30% // but, 앞에 문장이 없을 경우 제대로 학습 어려움.

**<word representation>** 

-word representation : 텍스트를 어떻게 수치화해서 input으로 넣을 수 있을까 설명. 머신러닝을 통해 좋은 언어 모델을 만들기 위해서는 **단어들 간의 관계**를 잘 나타내야 함. // 단어들의 순서는 전혀 고려하지 않고, 출현 빈도에 집중하여 수치화하는 방법

​      -단어 표현법 : local representation, continous representation

​			-local representation : 해당 단어만 보고 특정값을 통해 표현 // 특정 단어만 봄 // one-hot 			vector, n-gram, bag of words

​				-one-hot vector : 원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 				단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식. 				이렇게 표현된 벡터 : one-hot vector

​				-n-gram : 임의의 개수를 정하기 위한 기준. n개의 연속적인 단어 나열. n개의 단어 단위로 끊				어서 하나의 토큰으로 간주

​				-bag of words : 단어들을 하나의 가방에 넣음. 단어들의 순서는 전혀 고려하지 않고 출현 				**빈도**에만 집중해서 수치화하는 방법

​					-DTM[문서 단어 행렬] : bag of words로 표현한 여러 문장, 문서들을 행렬로 표현 // 공					간 낭비가 심하고, 단순 빈도 수 기반 접근임. 영어에서 the가 많다고 the가 중요한 단					어인가?

​					-TF-IDF : DTM을 보완한 방법. TF=단어의 빈도, IDF=역문서빈도 // 단어들의 빈도와 역 					문서 빈도를 사용하여 DTM 내의 단어들의 중요도를 가중치로 주는 방식 // **유사도, 					중요도**를 구하는데 사용 

​						-TF : 특정 문서 d에서 특정 단어 t의 등장 횟수(DTM에서 각 단어들이 가지던 값)  

​						-DF : 특정 단어 t가 등장한 문서의 횟수(여러 문서에서 단어 t가 포함된 문서의 개수)

​						-IDF : DF(t)에 반비례하는 값(특정 단어가 등장하지 않을 경우에 대비해 1을 더함으로 분						모가 0이 되는 상황을 방지)

​            -continuous representation : 주변을 참고하여 단어를 표현

​      -topic modeling : LSA, LDA

​		-LSA(Latent Semantic Analysis) [잠재 의미 분석] : topic modeling. 주제를 찾기 위한 방법으로 		제시되는 것이 LSA와 LDA. // SVD를 통해 DTM으로 표현된 잠재된 의미를 이끌어내는 방법. TF-		IDF에서 단순히 빈도만 표현하여 단어의 의미를 고려하지 못하는 한계를 극복. (잠재된 의미를 끌		어냄) 

​			-SVD : a라는 m*n행렬을 m*m 직교 행렬, m*n 대각 행렬, n*n 직교 행렬로 분해 // a라는 행렬을 더 			낮은 차원에서 설명+유사하게 만드는 것이 SVD의 목적 // Truncated SVD(대각 행렬 중 상위만 			남겨두는 방식) 

​		-LDA(Latent Dirichlet Allocation) [잠재 디레클레 할당] : 문서들은 topic들의 합으로 구성되고, 		topic들은 확률 분포에 기반하여 단어를 생성한다고 가정 // 과정 : 1. 문서에 사용할 단어의 개수 N		을 선정 2. 문서에 사용할 Topic의 혼합을 확률 분포에 기반하여 결정 3. 문서에 사용할 각 단어를		(1.topic 분포에서 사용할 topic을 선정 2. 선택한 topic에서 단어 출현 확률 분포에 따라 문서에 사		용할 단어 선정). // 이 과정을 역 추적하는 방식으로 수행 -> 1. 사용자는 알고리즘에게 토픽 개수 k		를 알려줌. 2. 모든 단어를 k개의 토픽 중 하나에 할당 3. 어떤 문서의 단어 w가 잘못된 토픽에 할당		되어 있고, 다른 단어들은 전부 잘 할당되어 있다고 가정하면, 두 가지 기준(p(topic t | document 		d) : 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율 // p(word w | topic t) : 각 토픽들 t에서 		해당 단어 w의 분포)에 따라 재할당. 4. 과정 3 반복

-LSA vs LDA 알고리즘 

​            -LSA : DTM을 차원 축소(SVD)해서 근접 단어들을 topic으로 묶음

​			-LDA : 단어가 특정 topic에 존재할 확률과 문서에 특정 topic이 존재할 확률의 결합확률로 추정			하여 topic을 추출 

<NLP metrics>

-NLP metrics : NLP는 번역, 문장 요약 등 다양한 것이 필요하기 때문에 더 잘 학습시키기 위해 각각에 맞는 것이 필요해서 존재. // 유사도, PPL(perplexity), BELU score, ROUGE score

​	-유사도 : 문서 간 얼마나 비슷한지를 파악하기 위해 // 대표적 : cosine similarity(벡터 간 코사인 각	도를 통해 벡터의 유사도를 계산. 가까울수록 1, 멀수록(반대의 의미) -1)

​	-PPL : test 데이터에 대해 빠르게 평가하는 내부 평가 방법(자기 문장들을 통해 평가) // PPL최소화 : 	문장의 확률을 최대화

​	-BELU score : PPL은 번역의 성능을 직접적으로 반영하는 수치라 보기엔 어려움. 기계 번역 결과와 	사람의 번역 결과가 얼마나 유사한지 비교(기계 번역의 성능이 얼마나 뛰어난가를 측정하기 위한 방	법) // 측정 기준은 n-gram에 기반 // 순서 : 1. 단어 개수로 측정. 2. 중복 단어 보정. 3.순서를 고려한 	n-gram으로 확장

​	-ROUGE score : 다양한 종류 존재. (ROUGE-N, ROUGE-L, ROUGE-S

​    	-ROUGE-N : n-gram으로 recall 점수를 측정

​		-Precision/Recall/f-score : f-score은 precision과 recall을 같이 합산.

​		-ROUGE-L : 두 문장 중 겹치는 가장 긴 n-gram으로 계산

​		-ROUGE-S : skip-gram(중간에 skip을 허용)을 허용해서 계산

